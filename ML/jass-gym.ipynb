{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, SupportsFloat\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import RenderFrame, ObsType, ActType\n",
    "\n",
    "from jass.game.const import *\n",
    "from jass.game.game_util import *\n",
    "from jass.game.game_state_util import *\n",
    "from jass.game.game_sim import GameSim\n",
    "from jass.game.game_state import GameState\n",
    "from jass.game.game_observation import GameObservation\n",
    "from jass.game.rule_schieber import RuleSchieber\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b854776672b633",
   "metadata": {},
   "source": [
    "- https://gymnasium.farama.org/content/basic_usage/\n",
    "- https://github.com/zmcx16/OpenAI-Gym-Hearts/blob/master/src/Hearts/Hearts.py\n",
    "- https://gymnasium.farama.org/environments/toy_text/blackjack/#references\n",
    "- https://www.youtube.com/watch?v=YLa_KkehvGw\n",
    "- https://github.com/bernhard-pfann/uno-card-game-rl/tree/main\n",
    "- https://towardsdatascience.com/tackling-uno-card-game-with-reinforcement-learning-fad2fc19355c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d4d8014df8d6e",
   "metadata": {},
   "source": [
    "Okay so if I understand correctly, the model can be pretty simple. Taking some features like trump, player, cards in trick, played cards, and hand and returning a softmax over the cards to play the next one.\n",
    "\n",
    "The big question is how to train it. A gym wrapper around the existing framework might help but maybe isn't even necessary because there is already a lot implemented with GameSim etc.\n",
    "\n",
    "In a first step, this can just be trained with PPO or something like that, which should be possible without any MCTS just by self play. Biggest issue here will be to determine the value of a given time step when the game hasn't ended yet.\n",
    "\n",
    "For an AlphaZero-like implementation, you'd apparently play games using MCTS and learn from that a value function? But it's still not supervised learning?\n",
    "\n",
    "The UNO example does a bare re-implementation of Q learning with Belman and epsilon-greedy moves. I think using a PyTorch implementation of such an algorithm would be better, but you have to do some understanding first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a2bf9b8280f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "INVALID_CARD_REWARD = -100  # maybe need to be tunable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bbdbae46b5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do different reward functions etc. later so it's tunable\n",
    "def get_rewards(state: GameState):\n",
    "    # returns our points - their points -> constant sum\n",
    "    player_team = team[state.player]\n",
    "    player_team_score = state.points[player_team] - state.points[1 - player_team]\n",
    "    return {p: (player_team_score if team[p] == player_team else -player_team_score) for p in range(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838462896a96f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_vector(obs: GameObservation, valid_cards: np.ndarray):\n",
    "    # todo, not needed yet, but you'll need a vector where all the features are contained.\n",
    "    # all the cards should be one-hot encoded so the played cards will already take 36x36,\n",
    "    # as there are 36 cards to play and each one could be one of 36.\n",
    "    # which card is currently being played could be a number between 0 and 1 as it's also\n",
    "    # a percentage of how far a game is along. Same for number cards in trick. This is btw. the same as min-max scaling.\n",
    "    # Trump should be one-hot encoded too though.\n",
    "    # The number of cards each player has in their hand (4x1, from 0 to 1).\n",
    "    # Trick winners should also be included in the features -> may have used all good cards\n",
    "    # Who declared trump should too -> probably has good cards for that suit/trump\n",
    "    # If player pushed (and who) -> probably has general cards, not good for any specific suit\n",
    "    # These player things can also be done one-hot so one 4 long vector for who declared, one for who pushed,\n",
    "    # one for who is me, one for who is my partner, one for who are my enemies, one for who won trick 0, one for who won trick 1, ..\n",
    "    # for the hand cards have all 9 possible hand cards one hot encoded (9x36) and also one binary encoded (multilabel, 1x36).\n",
    "    \n",
    "    # ps. could run a convolution on all the cards so that there are shared weights which extract some information on a specific card.\n",
    "    # this could be done by concating the trump on top of the cards (6 + 36 = 42), then concating all card stacks (at least 36 existing + 9 hand cards),\n",
    "    # then do the conv with stride 42. Multiple filters but conv to just 1 number per card stack? So youd get num_filters new features per hand.\n",
    "    # The extracted features can be concat to the card stacks again before stacking/flattening again.\n",
    "    v = np.array\n",
    "    np.array([\n",
    "        obs.player,\n",
    "        obs.nr_played_cards,\n",
    "        obs.nr_cards_in_trick,\n",
    "        obs.trump,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbfd611bd59da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JassEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.game: GameSim = GameSim(RuleSchieber())\n",
    "        self.current_observation: GameObservation = None\n",
    "        self.valid_actions: np.ndarray = None\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.game.state\n",
    "\n",
    "    @property\n",
    "    def player(self):\n",
    "        return self.state.player\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        dealer = options['dealer']\n",
    "\n",
    "        self.game.init_from_cards(dealer=dealer, hands=deal_random_hand(self.np_random))\n",
    "\n",
    "        self.valid_actions = self.game.rule.get_valid_actions_from_state(self.game.state)\n",
    "\n",
    "        self.current_observation = self.game.get_observation()\n",
    "\n",
    "        info = {'invalid': False}\n",
    "        return self.current_observation, info\n",
    "\n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ):\n",
    "        # action is simply an int with the action as specified by jass-kit itself.\n",
    "        terminated = False\n",
    "        info = {'invalid': False}\n",
    "        rewards = {i: 0 for i in range(4)}\n",
    "        curr_player = self.player\n",
    "        if self.valid_actions[action] != 1:\n",
    "            # player played an invalid card, don't update game\n",
    "            rewards[curr_player] = INVALID_CARD_REWARD\n",
    "            info['invalid'] = True\n",
    "        else:\n",
    "            self.game.action(action)\n",
    "            self.valid_actions = self.game.rule.get_valid_actions_from_state(self.game.state)\n",
    "            self.current_observation = self.game.get_observation()\n",
    "            terminated = self.game.is_done()\n",
    "            if terminated:\n",
    "                rewards = get_rewards(self.game.state)\n",
    "\n",
    "        info['rewards'] = rewards\n",
    "        truncated = False\n",
    "        return self.current_observation, rewards[curr_player], terminated, truncated, info\n",
    "\n",
    "    def render(self) -> None:\n",
    "        if self.render_mode != 'human':\n",
    "            raise ValueError(\"Invalid render mode\")\n",
    "\n",
    "        if self.state.trump == -1:\n",
    "            print(f\"Trump: to be defined, push {'NOT ' if self.state.forehand != -1 else ''}available\")\n",
    "        else:\n",
    "            print(f\"Trump: {self.state.trump}\")\n",
    "\n",
    "        to_str = convert_one_hot_encoded_cards_to_str_encoded_list\n",
    "        print(f\"Current player: {self.player}\")\n",
    "        print(f\"Their hand: {to_str(self.current_observation.hand)}\")\n",
    "        print(f\"Tricks done so far: {self.current_observation.nr_tricks}\")\n",
    "        print(f\"Cards in trick: {self.current_observation.nr_cards_in_trick}\")\n",
    "\n",
    "        partner = partner_player[self.player]\n",
    "        o1 = next_player[self.player]\n",
    "        o2 = next_player[partner]\n",
    "        print(f\"Partners hand: {to_str(self.state.hands[partner])}\")\n",
    "        print(f\"Opponent hand P{o1}: {to_str(self.state.hands[o1])}\")\n",
    "        print(f\"Opponent hand P{o2}: {to_str(self.state.hands[o2])}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac795f7b76124b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dicts(a, b):\n",
    "    # left join basically\n",
    "    return {i: a[i] + b[i] for i in a.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33134b4c5625f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(a, b):\n",
    "    # must have the same indices\n",
    "    for k in a.keys():\n",
    "        a[k] += b[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1d6697cc278a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randoms(n_episodes: int):\n",
    "    # for best balance, set n_episodes divisible by 4, then everyone starts the same number of times\n",
    "    from jass.agents.agent_random_schieber import AgentRandomSchieber\n",
    "    env = JassEnv()\n",
    "    players = [AgentRandomSchieber()] * 4  # reuses the same instance, which is fine for us\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, *_ = env.reset(seed=(42 if episode == 0 else None), options=dict(dealer=episode % 4))\n",
    "        done = False\n",
    "        scores = {i: 0 for i in range(4)}\n",
    "\n",
    "        # first ask for the trumps and submit them\n",
    "        # then really start playing\n",
    "        # you'll have to see how you can do this for actual training with PPO etc.\n",
    "        player = players[obs.player]\n",
    "        trump = player.action_trump(obs)\n",
    "        assert 0 <= trump <= MAX_TRUMP or trump == PUSH or trump == PUSH_ALT, \"Invalid trump selected; fix external trump selection!\"\n",
    "        obs, *_ = env.step(trump_to_full(trump))\n",
    "        if trump == PUSH or trump == PUSH_ALT:\n",
    "            player = players[obs.player]\n",
    "            trump = player.action_trump(obs)\n",
    "            assert 0 <= trump <= MAX_TRUMP, \"Invalid trump selected; fix external trump selection!\"\n",
    "            obs, *_ = env.step(trump_to_full(trump))\n",
    "\n",
    "        while not done:\n",
    "            player_id = obs.player\n",
    "            player = players[player_id]\n",
    "            action = player.action_play_card(obs)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            # scores[player_id] += reward\n",
    "            add_to_dict(scores, info['rewards'])\n",
    "            \n",
    "        print(f\"Episode {episode} -> Scores {scores}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f927e9e127e2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_randoms(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137393a176841d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ef3f436a922d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4G",
   "language": "python",
   "name": "dl4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
