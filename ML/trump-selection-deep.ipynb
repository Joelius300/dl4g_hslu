{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a39edc74e7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jass.game.const import card_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3052e507f68d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert X.shape[0] == y.shape[0], \"X y dim mismatch\"\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        assert False, \"Inefficient __getitem__ called\"\n",
    "        # return torch.tensor(self.X[item]), torch.tensor(trump_to_one_hot(self.y[item]))\n",
    "    \n",
    "    def __getitems__(self, items):\n",
    "        # linear layers and CrossEntropyLoss both need float tensors (in case of class probabilities).\n",
    "        # The CrossEntropyLoss apparently is more efficient if given the class indices instead of the class probabilities\n",
    "        # so no need to one-hot encode\n",
    "        return torch.FloatTensor(self.X[items]), torch.LongTensor(self.y[items])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bccfb3d8655a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_path: str, test_split: float, val_split: float, batch_size: int, num_workers: int, promising_users_path=None):\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.promising_users_path = promising_users_path\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data = None\n",
    "        self.promising_users = None\n",
    "        self.features = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        self.features = np.append(card_strings, ['FH'])\n",
    "        cols = np.append(self.features, ['user', 'trump'])\n",
    "        self.data = pd.read_csv(self.csv_path, header=None, names=cols)\n",
    "        \n",
    "        if self.promising_users_path:\n",
    "            # only use the promising users (what is promising is decided in the player-selection notebook)\n",
    "            # currently not using the scores, could maybe use them with something like this\n",
    "            # https://stackoverflow.com/a/77300557/10883465\n",
    "            # but definitely not just *sample_weights, maybe * (1 + sample_weights/2), to just give\n",
    "            # the good players a tiny importance boost.\n",
    "            self.promising_users = pd.read_csv(self.promising_users_path)\n",
    "            print(f\"Filtering trump selections from {self.promising_users.shape[0]} promising users.\")\n",
    "            self.data = self.data[self.data['user'].isin(self.promising_users['id'])]\n",
    "        \n",
    "        X = self.data[self.features].values\n",
    "        y = self.data['trump'].values\n",
    "        \n",
    "        # we need stratification, otherwise torch's random_split would work too\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_split, stratify=y, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=self.val_split, stratify=y_train, random_state=42)\n",
    "        \n",
    "        self.train_dataset = TrumpDataset(X_train, y_train)\n",
    "        self.val_dataset = TrumpDataset(X_val, y_val)\n",
    "        self.test_dataset = TrumpDataset(X_test, y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        # the default collate thinks __getitems__ returns a list of tuples that need to be stacked to get the batched values\n",
    "        # just like you would need to if you invoked __getitem__ multiple times and tried to batch that together.\n",
    "        # however, __getitems__ already returns the tensor with the items stacked so no need for additional processing.\n",
    "        # see https://pytorch.org/docs/stable/data.html#torch.utils.data._utils.collate.collate potentially\n",
    "        assert type(batch) == tuple and type(batch[0]) == torch.Tensor and type(batch[1]) == torch.Tensor, \"Did not get tensor from dataset, investigate and update collate\"\n",
    "        \n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016431d94d8e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpSelection(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        n_classes = 7\n",
    "        self.ll = nn.ModuleList([nn.Linear(input_dim, hidden_dim)] + [nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers-1)])\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.metrics = nn.ModuleDict(dict(\n",
    "            accuracy=torchmetrics.Accuracy('multiclass', num_classes=n_classes),\n",
    "            precision=torchmetrics.Precision('multiclass', num_classes=n_classes),\n",
    "            recall=torchmetrics.Recall('multiclass', num_classes=n_classes),\n",
    "            f1=torchmetrics.F1Score('multiclass', num_classes=n_classes),\n",
    "        ))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.ll:\n",
    "            x = l(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # no softmax here because of CrossEntropyLoss does that internally for better numerical stability\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        return self.step(\"train_\", batch)\n",
    "\n",
    "    def validation_step(self, batch, _batch_idx):\n",
    "        return self.step(\"val_\", batch)\n",
    "    \n",
    "    def test_step(self, batch, _batch_idx):\n",
    "        return self.step(\"test_\", batch)\n",
    "        \n",
    "    def step(self, prefix, batch):\n",
    "        X, y = batch\n",
    "        predictions = self(X)\n",
    "        loss = self.criterion(predictions, y)\n",
    "        self.log(prefix + \"loss\", loss)\n",
    "\n",
    "        # remember, prediction is still the logits.\n",
    "        # many of these metrics should be able to handle that\n",
    "        # but for efficiency and to be sure, let's do the softmax ourselves.\n",
    "        predictions = F.softmax(predictions, dim=-1)\n",
    "        self._log_and_update_metrics(prefix, predictions, y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _log_and_update_metrics(self, prefix, prediction, y):\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric(prediction, y)\n",
    "            self.log(prefix + name, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7577303ba8d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "def train(hidden_dim: int, n_layers: int, learning_rate: float, batch_size: int, epochs: int):\n",
    "    hparams = copy.deepcopy(locals())  # feels wrong but does the job\n",
    "    seed_everything(42)\n",
    "    dm = TrumpDataModule(\"./data/2018_10_18_trump.csv\", test_split=.2, val_split=.2, num_workers=4, batch_size=batch_size, promising_users_path=\"./data/promising_players.csv\")\n",
    "    input_dim = 36 + 1  # all cards + forehand\n",
    "    model = TrumpSelection(input_dim, hidden_dim, n_layers, learning_rate)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=epochs, callbacks=[LearningRateMonitor(logging_interval='step'), checkpoint_callback], profiler='simple', log_every_n_steps=5)\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hparams)\n",
    "    \n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e03525f3a63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(150, 8, 1e-3, 2000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354e47ed9ab8a6",
   "metadata": {},
   "source": [
    "The Graf heuristic is a regression from 36 values to 1 with a linear transformation (there are just 36 values that are multiplied with the 36 hand values and then summed to get the score). You could also say it's a linear transformation from 36 values to 6 values where there's one heuristic score per trump. At the end there's just an argmax, which could also just be a softmax. The only additional thing is the threshold that needs to be crossed otherwise it's Schieben.\n",
    "\n",
    "What I'm trying to say is, a network that can model this heuristic would be very simple and one that should perform better, would not need to be much larger. However, if the model is supposed to imitate human players, like it would be if we trained it supervised on this data, then it will learn different things.\n",
    "\n",
    "One approach could also be to pre-train a network with the graf heuristic (all possible card combinations with their respective graf heuristic choice can be generated). Then this network already has a good basis. It could then be fine-tuned on the historical data from swisslos, but maybe only on the very best performing players, because a lot less data is required since the performance is already good with just the heuristic.\n",
    "\n",
    "The only way to ensure one method actual outperforms another, you need to test the same card-playing-bot playing against each other with different trump selection methods, e.g. one team ISMCTS with Graf and one team ISMCTS with deep learning purely from historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae8b229e9167b9",
   "metadata": {},
   "source": [
    "Sidenote: With a complex architecture, this model will overfit quickly but as you'll see, the accuracy, recall, etc. are still increasing. This could be a side effect of the class imbalance or a sign that the model is getting more unsure. See also [this SO question](https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3c21691d97176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4G",
   "language": "python",
   "name": "dl4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
