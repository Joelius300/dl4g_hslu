{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a39edc74e7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jass.game.const import card_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535dc33a5097675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trump_to_one_hot(trump: int) -> np.ndarray:\n",
    "    if trump == 10:\n",
    "        trump = 6  # correct PUSH down to 6 (there are both cases in the dataset)\n",
    "    one_hot = np.zeros(4 + 2 + 1, dtype=int)  # 4 suits + une_ufe & obe_abe + push\n",
    "    one_hot[trump] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506c33364ae7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trump_to_one_hot(trumps: np.ndarray) -> np.ndarray:\n",
    "    # correct PUSH down to 6 (there are both cases in the dataset)\n",
    "    trumps[trumps == 10] = 6\n",
    "    one_hot = np.zeros((trumps.shape[0], 4 + 2 + 1), dtype=int)  # 4 suits + une_ufe & obe_abe + push\n",
    "    np.put_along_axis(one_hot, np.expand_dims(trumps, 1), 1, axis=1)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e600659e5375ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trump_to_one_hot(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa4f4c231885da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_to_one_hot(np.array([4, 5, 1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3052e507f68d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert X.shape[0] == y.shape[0], \"X y dim mismatch\"\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        assert False, \"Inefficient __getitem__ called\"\n",
    "        # return torch.tensor(self.X[item]), torch.tensor(trump_to_one_hot(self.y[item]))\n",
    "    \n",
    "    def __getitems__(self, items):\n",
    "        # linear layers and CrossEntropyLoss both need float tensors\n",
    "        return torch.FloatTensor(self.X[items]), torch.FloatTensor(trump_to_one_hot(self.y[items]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bccfb3d8655a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_path: str, test_split: float, val_split: float, batch_size: int, num_workers: int, promising_users_path=None):\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.promising_users_path = promising_users_path\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data = None\n",
    "        self.promising_users = None\n",
    "        self.features = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        self.features = np.append(card_strings, ['FH'])\n",
    "        cols = np.append(self.features, ['user', 'trump'])\n",
    "        self.data = pd.read_csv(self.csv_path, header=None, names=cols)\n",
    "        \n",
    "        if self.promising_users_path:\n",
    "            # only use the promising users (what is promising is decided in the player-selection notebook)\n",
    "            # currently not using the scores, could maybe use them with something like this\n",
    "            # https://stackoverflow.com/a/77300557/10883465\n",
    "            # but definitely not just *sample_weights, maybe * (1 + sample_weights/2), to just give\n",
    "            # the good players a tiny importance boost.\n",
    "            self.promising_users = pd.read_csv(self.promising_users_path)\n",
    "            self.data = self.data[self.data['user'] in self.promising_users['id']]\n",
    "        \n",
    "        X = self.data[self.features].values\n",
    "        y = self.data['trump'].values\n",
    "        \n",
    "        # we need stratification, otherwise torch's random_split would work too\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_split, stratify=y, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=self.val_split, stratify=y_train, random_state=42)\n",
    "        \n",
    "        self.train_dataset = TrumpDataset(X_train, y_train)\n",
    "        self.val_dataset = TrumpDataset(X_val, y_val)\n",
    "        self.test_dataset = TrumpDataset(X_test, y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        # the default collate thinks __getitems__ returns a list of tuples that need to be stacked to get the batched values\n",
    "        # just like you would need to if you invoked __getitem__ multiple times and tried to batch that together.\n",
    "        # however, __getitems__ already returns the tensor with the items stacked so no need for additional processing.\n",
    "        # see https://pytorch.org/docs/stable/data.html#torch.utils.data._utils.collate.collate potentially\n",
    "        assert type(batch) == tuple and type(batch[0]) == torch.Tensor and type(batch[1]) == torch.Tensor, \"Did not get tensor from dataset, investigate and update collate\"\n",
    "        \n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016431d94d8e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpSelection(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        n_classes = 7\n",
    "        self.ll = nn.ModuleList([nn.Linear(input_dim, hidden_dim)] + [nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers-1)])\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.metrics = nn.ModuleDict(dict(\n",
    "            accuracy=torchmetrics.Accuracy('multiclass', num_classes=n_classes),\n",
    "            precision=torchmetrics.Precision('multiclass', num_classes=n_classes),\n",
    "            recall=torchmetrics.Recall('multiclass', num_classes=n_classes),\n",
    "            f1=torchmetrics.F1Score('multiclass', num_classes=n_classes),\n",
    "        ))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.ll:\n",
    "            x = l(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # no softmax here because of CrossEntropyLoss does that internally for better numerical stability\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        return self.step(\"train_\", batch)\n",
    "\n",
    "    def validation_step(self, batch, _batch_idx):\n",
    "        return self.step(\"val_\", batch)\n",
    "    \n",
    "    def test_step(self, batch, _batch_idx):\n",
    "        return self.step(\"test_\", batch)\n",
    "        \n",
    "    def step(self, prefix, batch):\n",
    "        X, y = batch\n",
    "        predictions = self(X)\n",
    "        loss = self.criterion(predictions, y)\n",
    "        self.log(prefix + \"loss\", loss)\n",
    "\n",
    "        # remember, prediction is still the logits.\n",
    "        # many of these metrics should be able to handle that\n",
    "        # but for efficiency and to be sure, let's do the softmax ourselves.\n",
    "        predictions = F.softmax(predictions, dim=-1)\n",
    "        self._log_and_update_metrics(prefix, predictions, y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _log_and_update_metrics(self, prefix, prediction, y):\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric(prediction, y)\n",
    "            self.log(prefix + name, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7577303ba8d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "def train(hidden_dim: int, n_layers: int, learning_rate: float, batch_size: int, epochs: int):\n",
    "    hparams = copy.deepcopy(locals())  # feels wrong but does the job\n",
    "    seed_everything(42)\n",
    "    dm = TrumpDataModule(\"./data/2018_10_18_trump.csv\", test_split=.2, val_split=.2, num_workers=4, batch_size=batch_size, promising_users_path=\"./data/promising_players.csv\")\n",
    "    input_dim = 36 + 1  # all cards + forehand\n",
    "    model = TrumpSelection(input_dim, hidden_dim, n_layers, learning_rate)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=epochs, callbacks=[LearningRateMonitor(logging_interval='step'), checkpoint_callback], profiler='simple', log_every_n_steps=5)\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hparams)\n",
    "    \n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e03525f3a63e7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train(250, 10, 1e-3, 2000, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3c21691d97176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4G",
   "language": "python",
   "name": "dl4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
