{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a39edc74e7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jass.game.const import card_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3052e507f68d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert X.shape[0] == y.shape[0], \"X y dim mismatch\"\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        assert False, \"Inefficient __getitem__ called\"\n",
    "        # return torch.tensor(self.X[item]), torch.tensor(trump_to_one_hot(self.y[item]))\n",
    "    \n",
    "    def __getitems__(self, items):\n",
    "        # linear layers and CrossEntropyLoss both need float tensors (in case of class probabilities).\n",
    "        # The CrossEntropyLoss apparently is more efficient if given the class indices instead of the class probabilities\n",
    "        # so no need to one-hot encode\n",
    "        return torch.FloatTensor(self.X[items]), torch.LongTensor(self.y[items])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bccfb3d8655a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_path: str, test_split: float, val_split: float, batch_size: int, num_workers: int):\n",
    "        super().__init__()\n",
    "        self.csv_path = csv_path\n",
    "        self.test_split = test_split\n",
    "        self.val_split = val_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data = None\n",
    "        self.promising_users = None\n",
    "        self.features = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        self.features = np.append(card_strings, ['FH'])\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        X = self.data[self.features].values\n",
    "        fh = self.data['FH'].values\n",
    "        y = self.data['trump'].values\n",
    "        \n",
    "        # we need stratification, otherwise torch's random_split would work too\n",
    "        X_train, X_test, y_train, y_test, fh_train, _ = train_test_split(X, y, fh, test_size=self.test_split, stratify=(fh * 10 + y), random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=self.val_split, stratify=(fh_train * 10 + y_train), random_state=42)\n",
    "        \n",
    "        self.train_dataset = TrumpDataset(X_train, y_train)\n",
    "        self.val_dataset = TrumpDataset(X_val, y_val)\n",
    "        self.test_dataset = TrumpDataset(X_test, y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, collate_fn=self.collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        # the default collate thinks __getitems__ returns a list of tuples that need to be stacked to get the batched values\n",
    "        # just like you would need to if you invoked __getitem__ multiple times and tried to batch that together.\n",
    "        # however, __getitems__ already returns the tensor with the items stacked so no need for additional processing.\n",
    "        # see https://pytorch.org/docs/stable/data.html#torch.utils.data._utils.collate.collate potentially\n",
    "        assert type(batch) == tuple and type(batch[0]) == torch.Tensor and type(batch[1]) == torch.Tensor, \"Did not get tensor from dataset, investigate and update collate\"\n",
    "        \n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c5c883d3d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jass.game.game_util import get_cards_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11218e35952e2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpGrafDataModule(pl.LightningDataModule):\n",
    "    # uses the generated and then balanced graf dataset\n",
    "    # could've just generated samples on the fly but then balancing and train/val splitting would be more complex\n",
    "    def __init__(self, train_path: str, val_path: str, batch_size: int, num_workers: int):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.features = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        cols = [f\"c{i}\" for i in range(1, 10)] + ['fh', 'trump']\n",
    "        train_df = pd.read_parquet(self.train_path, columns=cols)\n",
    "        val_df = pd.read_parquet(self.val_path, columns=cols)\n",
    "        X_train, y_train = self.to_one_hot(train_df)\n",
    "        X_val, y_val = self.to_one_hot(val_df)\n",
    "        \n",
    "        self.train_dataset = TrumpDataset(X_train, y_train)\n",
    "        self.val_dataset = TrumpDataset(X_val, y_val)\n",
    "        \n",
    "    def to_one_hot(self, df: pd.DataFrame):\n",
    "        non_card_cols = ['fh', 'trump']\n",
    "        fh_trump = df[non_card_cols]\n",
    "        cards = df.drop(non_card_cols, axis=1).values\n",
    "        # takes a while, but dataset isn't that big anymore, this is doable\n",
    "        one_hot = np.apply_along_axis(get_cards_encoded, 1, cards)\n",
    "        return np.append(one_hot, np.expand_dims(fh_trump['fh'].values, axis=1), axis=1), fh_trump['trump'].values\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, collate_fn=self.collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, collate_fn=self.collate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        # the default collate thinks __getitems__ returns a list of tuples that need to be stacked to get the batched values\n",
    "        # just like you would need to if you invoked __getitem__ multiple times and tried to batch that together.\n",
    "        # however, __getitems__ already returns the tensor with the items stacked so no need for additional processing.\n",
    "        # see https://pytorch.org/docs/stable/data.html#torch.utils.data._utils.collate.collate potentially\n",
    "        assert type(batch) == tuple and type(batch[0]) == torch.Tensor and type(batch[1]) == torch.Tensor, \"Did not get tensor from dataset, investigate and update collate\"\n",
    "        \n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016431d94d8e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrumpSelection(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        n_classes = 7\n",
    "        self.ll = nn.ModuleList([nn.Linear(input_dim, hidden_dim)] + [nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers-1)])\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.metrics = nn.ModuleDict(dict(\n",
    "            accuracy=torchmetrics.Accuracy('multiclass', num_classes=n_classes),\n",
    "            precision=torchmetrics.Precision('multiclass', num_classes=n_classes),\n",
    "            recall=torchmetrics.Recall('multiclass', num_classes=n_classes),\n",
    "            f1=torchmetrics.F1Score('multiclass', num_classes=n_classes),\n",
    "        ))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.ll:\n",
    "            x = l(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # no softmax here because of CrossEntropyLoss does that internally for better numerical stability\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        return self.step(\"train_\", batch)\n",
    "\n",
    "    def validation_step(self, batch, _batch_idx):\n",
    "        return self.step(\"val_\", batch)\n",
    "    \n",
    "    def test_step(self, batch, _batch_idx):\n",
    "        return self.step(\"test_\", batch)\n",
    "        \n",
    "    def step(self, prefix, batch):\n",
    "        X, y = batch\n",
    "        predictions = self(X)\n",
    "        loss = self.criterion(predictions, y)\n",
    "        self.log(prefix + \"loss\", loss)\n",
    "\n",
    "        # remember, prediction is still the logits.\n",
    "        # many of these metrics should be able to handle that\n",
    "        # but for efficiency and to be sure, let's do the softmax ourselves.\n",
    "        predictions = F.softmax(predictions, dim=-1)\n",
    "        self._log_and_update_metrics(prefix, predictions, y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _log_and_update_metrics(self, prefix, prediction, y):\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric(prediction, y)\n",
    "            self.log(prefix + name, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fda43aaf0c4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 36 + 1  # all cards + forehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e248a4d91fa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def train_dm(dm: pl.LightningDataModule, hidden_dim: int, n_layers: int, learning_rate: float, batch_size: int, epochs: int):\n",
    "    hparams = copy.deepcopy(locals())  # feels wrong but does the job\n",
    "    seed_everything(42)\n",
    "    model = TrumpSelection(input_dim, hidden_dim, n_layers, learning_rate)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "    # patience is number of epochs of being worse before stopping. actually it's number of val checks, but we check val once per epoch.\n",
    "    # with this we slightly overtrain the model as the val_accuracy continues to rise for a bit while the val_loss is already increasing again.\n",
    "    # see https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy\n",
    "    early_stopping = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=epochs, callbacks=[LearningRateMonitor(logging_interval='step'), checkpoint_callback, early_stopping], profiler='simple', log_every_n_steps=5)\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hparams)\n",
    "    trainer.logger.log_graph(model)\n",
    "    \n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369df5b0d227aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model: pl.LightningModule, dm: pl.LightningDataModule, learning_rate: float, batch_size: int, epochs: int, early_stop_patience: int):\n",
    "    hparams = copy.deepcopy(locals())  # feels wrong but does the job\n",
    "    del hparams['model']\n",
    "    del hparams['dm']\n",
    "    seed_everything(42)\n",
    "\n",
    "    # if fine-tuning on imbalanced dataset, use f1 instead of accuracy\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "    early_stopping = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=early_stop_patience)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=epochs, callbacks=[LearningRateMonitor(logging_interval='step'), checkpoint_callback, early_stopping], profiler='simple', log_every_n_steps=1)\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hparams)\n",
    "    # trainer.logger.log_graph(model)\n",
    "    \n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c088bb6be5e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(hidden_dim: int, n_layers: int, learning_rate: float, batch_size: int, epochs: int):\n",
    "    dm = TrumpGrafDataModule(\"./data/graf-dataset-balanced/train/\", \"./data/graf-dataset-balanced/val/\", num_workers=4, batch_size=batch_size)\n",
    "    train_dm(dm, hidden_dim, n_layers, learning_rate, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d81110afb6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(checkpoint_path: str, learning_rate: float, batch_size: int, epochs: int, early_stop_patience: int):\n",
    "    model = TrumpSelection.load_from_checkpoint(checkpoint_path)\n",
    "    dm = TrumpDataModule(\"./data/trump_top250_balanced.csv\", test_split=.2, val_split=.2, num_workers=4, batch_size=batch_size)\n",
    "    finetune_model(model, dm, learning_rate, batch_size, epochs, early_stop_patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e03525f3a63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    hidden_dim = 50,\n",
    "    n_layers = 2,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 15000,\n",
    "    epochs = 100,\n",
    ")\n",
    "\n",
    "pre_train(**hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af7a82a4806",
   "metadata": {},
   "source": [
    "_The_ model was pre-trained on the graf dataset for 50 epochs and got a val_accuracy of 1. lr=1e-4. Checkpoint version_36\n",
    "\n",
    "It was then fine-tuned for 3894 epochs, before early stopping kicked in with patience = 250. lr was 1e-6. It reached a val_accuracy of 77%, but it was clear that progress has not yet stagnated. Checkpoint version_45\n",
    "\n",
    "I then continued fine-tuning from that new checkpoint with lr 1e-7, max_epoch 10000 and early_stopping_patience=1000. The learning rate was probably a bit small because it progressed very slowly, doing the entire 10k epochs in 1.2h and getting a val_accuracy of 79%. Checkpoint version_46.\n",
    "\n",
    "Progress still hasn't stagnated, so I'll start it again from here but with a slightly larger learning rate (5 times as large, logarithmic middle of 1e-6 and 1e-7). Also upped the epochs to 20k and decreased the early stopping to 750. Okay that didn't do what I'd hoped for as it didn't stagnate again and early stopping kicked in pretty soon.\n",
    "\n",
    "One last try, from 46 again, lr 1e-7 and patience on 1000. Checkpoint version_48. This got 79.6% Validation accuracy and seems to not be fully stagnated but fact of the matter is, that 1k epoch patience wasn't enough to keep going, so I'll call it here.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Try with a more complex model, but I suspect it won't be very beneficial.\n",
    "- Once done with tuning, run the testing. Note, this will only give a score of how well it imitates the top 250 players, not how good it's playing (the best chess bots also don't play like humans; I'd love to use this argument but if it isn't learning trump selection from self-play later on, this is an invalid argument).\n",
    "- Extract the model definition to a python file, so it can be imported and used elsewhere\n",
    "- Write an agent that uses the model definition and a checkpoint path to predict the best trump. Fallback can be graf if invalid but that hopefully doesn't happen.\n",
    "- Compare this agent with the graf agent and potentially with another model agent, where the model learned only on the training data, without pre-training. The card playing stategy has to be the same of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908d9ede603646",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    # checkpoint_path = \"./lightning_logs/version_36/checkpoints/epoch=50-step=65535.ckpt\",\n",
    "    # checkpoint_path = \"./lightning_logs/version_45/checkpoints/epoch=3644-step=10935.ckpt\",\n",
    "    checkpoint_path = \"./lightning_logs/version_46/checkpoints/epoch=9674-step=29025.ckpt\",\n",
    "    learning_rate = 1e-7,\n",
    "    batch_size = 3000,\n",
    "    epochs = 20000,\n",
    "    early_stop_patience=1000,\n",
    ")\n",
    "\n",
    "fine_tune(**hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354e47ed9ab8a6",
   "metadata": {},
   "source": [
    "The Graf heuristic is a regression from 36 values to 1 with a linear transformation (there are just 36 values that are multiplied with the 36 hand values and then summed to get the score). You could also say it's a linear transformation from 36 values to 6 values where there's one heuristic score per trump. At the end there's just an argmax, which could also just be a softmax. The only additional thing is the threshold that needs to be crossed otherwise it's Schieben.\n",
    "\n",
    "What I'm trying to say is, a network that can model this heuristic would be very simple and one that should perform better, would not need to be much larger. However, if the model is supposed to imitate human players, like it would be if we trained it supervised on this data, then it will learn different things.\n",
    "\n",
    "One approach could also be to pre-train a network with the graf heuristic (all possible card combinations with their respective graf heuristic choice can be generated). Then this network already has a good basis. It could then be fine-tuned on the historical data from swisslos, but maybe only on the very best performing players, because a lot less data is required since the performance is already good with just the heuristic. A big advantage here would also be that we can downsample to get balanced data, both for pre-training (there's a lot more data) and for fine-tuning (a lot less data is needed).\n",
    "\n",
    "The only way to ensure one method actual outperforms another, you need to test the same card-playing-bot playing against each other with different trump selection methods, e.g. one team ISMCTS with Graf and one team ISMCTS with deep learning purely from historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae8b229e9167b9",
   "metadata": {},
   "source": [
    "Sidenote: With a complex architecture, this model will overfit quickly but as you'll see, the accuracy, recall, etc. are still increasing. This could be a side effect of the class imbalance or a sign that the model is getting more unsure. See also [this SO question](https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3c21691d97176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4G",
   "language": "python",
   "name": "dl4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
